---
title: "STAT482_final"
author: "Sang Chan Lee"
date: "4/25/2020"
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo = FALSE}
library(glmnet)
library(caret)
library(BART)
library(stats)
library(mclust)
library(factoextra)
```

# Setup and data import
```{r, echo = FALSE}
{
  rm(list=ls())
  setwd("/Users/sangchanlee/Downloads/election_combined/finalResult")
  load(file = 'data.Rdata')
  pred2018 = read.csv(file = "pred2018.csv")
  pred2014 = read.csv(file = "pred2014.csv")
  load(file = 'logOdds_dem2014.Rdata')
}
```

# data description
There are total 61 covariates and 850 counties from Swing States, which are Colorado, Florida, Iowa, Michigan, Minnesota, Nevada, New Hampshire, North Carolina, Ohio, Pennsylvania, Virginia, and Wisconsin. For variable selection, Lasso regression is conducted with largest value of lambda such that error is within 1 standard error of the minimum. As a result, 11 variables are selected. Log odds ratio, $log(\frac{p(x)}{1-p(x)})$ for the response variable is used for non-parametric regression and 11 covariates are standardized.   

# Lasso Regression 
Lasso regression is a type of linear regression that uses shrinkage using L1 Regularization. This particular type of regression is well-suited for models showing high levels of muticollinearity or when you want to automate certain parts of model selection, like variable selection/parameter elimination. The goal of the lasso algorithm is to minimize the quantity, 
$$(\sum_{i=1}^{n} y_i - \beta_0 - \sum_{j=1}^{p} \beta_jx_{ij})^2 + \lambda\sum_{j=1}^{p}=|\beta_j| = RSS + \lambda||\beta||_1 $$


# Partitioning data 
Data is partitioned into train(75%) and test(25%) data sets. 
```{r}
set.seed(101)
indexing <- createDataPartition(y = data$y, p = 0.75,list = FALSE)
data_train = data[indexing,]
data_test = data[-indexing,]

```



# K-nearest neighbor 
The tuning parameter k=23 is used for K-nearest neighbor by Cross Validation. 
```{r}
knn_fit = train(y~., 
                data = data_train, 
                trControl = trainControl(method = 'repeatedcv', number = 10, repeats = 3), 
                method = 'knn', 
                tuneLength = 10
                )
knn_pred = predict(knn_fit, newdata = data_test)
knn_RMSE = sqrt(mean((data_test$y - knn_pred)^2))

```


# Random Forest 
The tuning parameters, the number of trees and number of variables randomly sampled as candiates at each split are 500 and 6 respectively by Cross Validation. 
```{r}
rf_fit = train(y~., 
               data = data_train, 
               trControl = trainControl(method = 'repeatedcv', number = 10, repeats = 3), 
               method = 'rf', 
               tuneLength = 5
               )
rf_pred = predict(rf_fit, newdata = data_test)
rf_RMSE = sqrt(mean((data_test$y - rf_pred)^2))

```


# Gradient Boost 
The tuning parameters, the number of trees is 50, the maximum depth of each tree is 5, and shrinkage is 0.1 are used by Cross Validation. 
```{r}
boost.fit <- train(y ~ ., 
                   data = data_train, 
                   method = "gbm", 
                   trControl = trainControl(method = 'repeatedcv', number = 10, repeats = 3), 
                   verbose = FALSE,
                   tuneLength = 10
                   ) 
boost_pred = predict(boost.fit, newdata = data_test)
boost_RMSE = sqrt(mean((data_test$y - boost_pred)^2))

```

# Baysian Additive Regression Trees
```{r}
bart_fit = gbart(x.train = data_train[,-1], y.train = data_train$y, type = 'wbart')
bart_pred = predict(bart_fit, newdata = data_test[,-1])

pred.mean=apply(bart_pred,2,mean)
bart_RMSE = sqrt(mean((data_test$y - pred.mean)^2))
```

# RMSE from knn, random forest, gradient boost and bart 
Among four non-parametric regression models, Gradient Boost has the lowest RMSE. Thus it is used for the prediction. 
```{r}
RMSE = cbind(knn_RMSE, rf_RMSE, boost_RMSE, bart_RMSE)
RMSE

```

# Data preprocessiong for prediction 
```{r}
data2018 = pred2018[,c(-1,-2,-3)]
data2014 = pred2014[,c(-1,-2,-3)]

for (i in 1:ncol(data2018)){
  data2018[,i] = as.numeric(data2018[,i])
}


data2018 = apply(data2018, 2, scale)
data2014 = apply(data2014, 2, scale)

data_pred = data2018 - data2014
colnames(data_pred) = colnames(data)[-1]
```


# 2020 prediction 
```{r}
pred = predict(boost.fit, newdata = data_pred)
logOdds_dem2018 = pred + logOdds_dem2014

prob2018 = exp(logOdds_dem2018) / (1+exp(logOdds_dem2018))

aa = c()
for (i in 1:length(prob2018)){
  if (prob2018[i] > 0.5){
    aa[i] = 0
  }
  else
    aa[i] = 1
}

# 0 for dem and 1 for gop 
vote_pred = data.frame(County = pred2018[,2], State = pred2018[,3], pred = aa)
vote_pred[1:10,]
```



# Principal Component Analysis 
For reduction of dimensionality of data, Principal Component Analysis(PCA) is conducted and 6 principal components are used so that 97.49% of variation is explained. 
```{r}
pca_out = prcomp(data2018)
summary(pca_out)$importance[3,]

data_cluster = pca_out$x[,1:6] 


```

# K-mean clustering 
By Siluhouette method, it is suggested to use two clusters for K-mean clustering model. In comparison to prediction from Gradient Boost, 784 counties have the same result. 
```{r}
kmean_fit <- kmeans(data, centers = 2, nstart = 10) 
plot(data_cluster,col=kmean_fit$cluster,cex=1.5,pch=16,lwd=2)


reg_kmean = data.frame(aa, bb = kmean_fit$cluster-1)
length(which(aa == kmean_fit$cluster-1))
```





# Hierarchical clustering
By Siluhouette method, it is suggested to use two clusters for hierarchical clustering model. In comparison to prediction from Gradient Boost, 735 counties have the same result. 
```{r}
hclust_ward = hcut(data_cluster, k = 2, hc_method = 'ward.D')

plot_ward = fviz_dend(hclust_ward, rect = TRUE, cex = 0.5,
          k_colors = c("#00AFBB","#2E9FDF"))
plot_ward

red_h = data.frame(aa, bb = hclust_ward$cluster -1)
length(which(aa == hclust_ward$cluster -1))

```












