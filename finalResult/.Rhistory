aa = aa[c(-750, -752, -753),]
aa = aa[c(-756, -759),]
aa = aa[c(-758, -763),]
aa = aa[c(-768, -776),]
aa = aa[c(-776, -777),]
d = c()
for (i in 1:nrow(aa)){
d[i] = aa$County[i] != pred2018$County[i]
}
d
ccc = read.csv(file = '/Users/sangchanlee/Downloads/election_combined/finalResult/data_pred2014.csv')
ccc = t(ccc)
colnames(ccc) = ccc[1,]
ccc = as.data.frame(ccc)
ccc = ccc[-1,]
ccc = separate(ccc, Geography, sep = ',', into = c("County", "State"))
ccc = ccc %>% arrange(State, County)
which(ccc$County == 'Richmond County')
richmond = ccc[788,]
aa[757,] = richmond
d = c()
for (i in 1:nrow(aa)){
d[i] = aa$County[i] != pred2018$County[i]
}
d
data_pred2014 = aa
write.csv(data_pred2014, file = "data_pred2014.csv")
setwd"/Users/sangchanlee/Downloads/election_combined/finalResult"
setwd(/Users/sangchanlee/Downloads/election_combined/finalResult)
setwd("/Users/sangchanlee/Downloads/election_combined/finalResult")
write.csv(data_pred2014, file = "data_pred2014.csv")
setwd("/Users/sangchanlee/Downloads/election_combined/finalResult")
setwd("/Users/sangchanlee/Downloads")
write.csv(data_pred2014, file = "data_pred2014.csv")
setwd("/Users/sangchanlee/Downloads")
write.csv(aa, file = "aa.csv")
setwd("/Users/sangchanlee/Downloads/election_combined/finalResult")
load(file = 'data.Rdata')
pred2018 = read.csv(file = "data_pred2018.csv")
pred2014 = read.csv(file = "data_pred2014.csv")
set.seed(101)
indexing <- createDataPartition(y = data$y, p = 0.75,list = FALSE)
data_train = data[indexing,]
data_test = data[-indexing,]
knn_fit = train(y~.,
data = data_train,
trControl = trainControl(method = 'repeatedcv', number = 10, repeats = 3),
method = 'knn',
tuneLength = 10
)
knn_pred = predict(knn_fit, newdata = data_test)
knn_RMSE = sqrt(mean((data_test$y - knn_pred)^2))
View(pred2014)
View(pred2018)
setwd("/Users/sangchanlee/Downloads/election_combined/finalResult")
load(file = 'data.Rdata')
pred2018 = read.csv(file = "data_pred2018.csv")
pred2014 = read.csv(file = "data_pred2014.csv")
set.seed(101)
indexing <- createDataPartition(y = data$y, p = 0.75,list = FALSE)
data_train = data[indexing,]
data_test = data[-indexing,]
View(pred2014)
pred2018[,-1:-4] - pred2014[,-1:-4]
pred2018[,c(-1,-2,-3,-4)] - pred2014[,(-1,-2,-3,-4)]
pred2018[,c(-1,-2,-3,-4)]
pred2014[,(-1,-2,-3,-4)]
pred2014[,(-1,-2,-3,-4)]
pred2018[,c(-1,-2,-3,-4)] - pred2014[,c(-1,-2,-3,-4)]
pred2014[,c(-1,-2,-3,-4)]
View(pred2014)
View(pred2018)
pred2018 = read.csv(file = "data_pred2018.csv")
pred2014 = read.csv(file = "data_pred2014.csv")
X2018 = pred2018[,c(-1,-2,-3)]
X2018
X2018 = scale(X2018, 2, scale)
X2018 = apply(X2018, 2, scale)
class(X2018)
class(X2018[,1])
class(X2018[,2])
class(X2018[,3])
for (i in 1:ncol(X2018)){
X2018[,i] = as.numeric(X2018[,i])
}
X2018 = apply(X2018, 2, scale)
X2018 = apply(X2018, 2, scale)
X2018
setwd("/Users/sangchanlee/Downloads/election_combined/finalResult")
load(file = 'data.Rdata')
pred2018 = read.csv(file = "data_pred2018.csv")
pred2014 = read.csv(file = "data_pred2014.csv")
X2018 = pred2018[,c(-1,-2,-3)]
for (i in 1:ncol(X2018)){
X2018[,i] = as.numeric(X2018[,i])
}
X2018 = apply(X2018, 2, scale)
X2014 = pred2014[,c(-1,-2,-3)]
for (i in 1:ncol(X2014)){
X2014[,i] = as.numeric(X2014[,i])
}
X2014 = apply(X2014, 2, scale)
X2018 - X2014
pred2018
data_pred2018 = data.frame(id = pred2018[,1], County = pred2018[,2], State = pred2018[,3], X2018)
View(data_pred2018)
data_pred2014 = data.frame(id = pred2014[,1], County = pred2014[,2], State = pred2014[,3], X2014)
write.csv(pred2018, file = "pred2018.csv")
write.csv(pred2014, file = "pred2014.csv")
setwd("/Users/sangchanlee/Downloads/election_combined/finalResult")
load(file = 'data.Rdata')
pred2018 = read.csv(file = "pred2018.csv")
pred2014 = read.csv(file = "pred2014.csv")
set.seed(101)
indexing <- createDataPartition(y = data$y, p = 0.75,list = FALSE)
data_train = data[indexing,]
data_test = data[-indexing,]
View(data)
View(pred2014)
colnames(data)
colnames(pred2018)
View(data)
View(pred2014)
View(pred2014)
pred2018 = pred2018[,-6]
pred2014 = pred2014[,-6]
write.csv(pred2018, file = "pred2018.csv")
write.csv(pred2014, file = "pred2014.csv")
setwd("/Users/sangchanlee/Downloads/election_combined/finalResult")
load(file = 'data.Rdata')
pred2018 = read.csv(file = "pred2018.csv")
pred2014 = read.csv(file = "pred2014.csv")
set.seed(101)
indexing <- createDataPartition(y = data$y, p = 0.75,list = FALSE)
data_train = data[indexing,]
data_test = data[-indexing,]
knn_fit = train(y~.,
data = data_train,
trControl = trainControl(method = 'repeatedcv', number = 10, repeats = 3),
method = 'knn',
tuneLength = 10
)
knn_pred = predict(knn_fit, newdata = data_test)
knn_RMSE = sqrt(mean((data_test$y - knn_pred)^2))
rf_fit = train(y~.,
data = data_train,
trControl = trainControl(method = 'repeatedcv', number = 10, repeats = 3),
method = 'rf',
tuneLength = 5
)
rf_pred = predict(rf_fit, newdata = data_test)
rf_RMSE = sqrt(mean((data_test$y - rf_pred)^2))
boost.fit <- train(y ~ .,
data = data_train,
method = "gbm",
trControl = trainControl(method = 'repeatedcv', number = 10, repeats = 3),
verbose = FALSE,
tuneLength = 10
)
boost_pred = predict(boost.fit, newdata = data_test)
boost_RMSE = sqrt(mean((data_test$y - boost_pred)^2))
RMSE = cbind(knn_RMSE, rf_RMSE, boost_RMSE, bart_RMSE)
RMSE
RMSE = cbind(knn_RMSE, rf_RMSE, boost_RMSE, bart_RMSE)
bart_pred = predict(bart_fit, newdata = data_test[,-1])
pred.mean=apply(bart_pred,2,mean)
bart_fit = gbart(x.train = data_train[,-1], y.train = data_train$y, type = 'wbart')
bart_pred = predict(bart_fit, newdata = data_test[,-1])
pred.mean=apply(bart_pred,2,mean)
bart_RMSE = sqrt(mean((data_test$y - pred.mean)^2))
RMSE = cbind(knn_RMSE, rf_RMSE, boost_RMSE, bart_RMSE)
RMSE
View(pred2014)
View(pred2018)
colnames(pred2014)
colnames(pred2018)
data2018 = pred2018[,c(-1,-2,-3)]
data2014 = pred2014[,c(-1,-2,-3)]
View(data2018)
data2018 - data2014
data2018 = apply(data2018, 2, scale)
data2014 = apply(data2014, 2, scale)
data2018 = apply(data2018, 2, scale)
data2014 = apply(data2014, 2, scale)
data2018 = apply(data2018, 2, scale)
data2014 = apply(data2014, 2, scale)
for (i in 1:ncol(data2018)){
data2018[,i] = as.numeric(data2018[,i])
}
data2018 = apply(data2018, 2, scale)
data2014 = apply(data2014, 2, scale)
data2018
pred2018 = read.csv(file = "pred2018.csv")
pred2014 = read.csv(file = "pred2014.csv")
data2018 = pred2018[,c(-1,-2,-3)]
data2014 = pred2014[,c(-1,-2,-3)]
for (i in 1:ncol(data2018)){
data2018[,i] = as.numeric(data2018[,i])
}
data2018 = apply(data2018, 2, scale)
data2014 = apply(data2014, 2, scale)
data2018 - data2014
data_pred = data2018 - data2014
pred = predict(boost.fit, newdata = data_pred)
colnames(data)
colnames(data_pred)
colnames(data)[,-1]
colnames(data)[-1]
colnames(data_pred) = colnames(data)[-1]
pred = predict(boost.fit, newdata = data_pred)
colnames(data)[-1]
pred
plot(density(pred))
max(pred)
logOdds_dem2016
setwd("/Users/sangchanlee/Downloads/election_combined")
sang_election = read.csv(file = "election_couty_cleaned.csv")
load(file = "full_data.Rdata")
# full_data: 2016 - 2012
full_data = full_data[-632,]
sang_election= sang_election[-623,]
# matching to common county
m1 = (which(full_data[,3] %in% sang_election[,4]))
data_feature_match = full_data[m1,]
data_feature_match = data_feature_match %>% arrange(State, Geography)
m2 = (which(sang_election[,4] %in% full_data[,3]))
data_response_match = sang_election[m2,]
data_response_match = sang_election[-729,]
data_response_match = data_response_match %>% arrange(state, county_nam)
states = c(rep('Colorado', 64), rep('Florida', 67), rep('Iowa', 99), rep('Michigan', 83), rep('Minnesota', 87),
rep('North Carolina', 100), rep('New Hampshire', 10), rep('Nevada', 17), rep('Ohio', 88),
rep('Pennsylvania', 66), rep('Virginia', 99), rep('Wisconsin', 72))
data_response_match$state = states
data_response_match = data_response_match %>% arrange(state, county_nam)
data_response_match = data_response_match[-691,]
data_feature_match = data_feature_match[-713,]
b = c()
for (i in 1:nrow(data_feature_match)){
b[i] = data_feature_match$Geography[i] != data_response_match$county_nam[i]
}
b
data_feature_match = data_feature_match[-620,]
data_response_match = data_response_match[-620,]
# extracting response for common county
data_response_match = data.frame(data_response_match[,3:4] ,data_response_match[,59:74])
# log odds
logOdds_dem2016 = log(data_response_match$pct_dem_16/(1-data_response_match$pct_dem_16))
logOdds_dem2012 = log(data_response_match$pct_dem_12/(1-data_response_match$pct_dem_12))
logOdds_dem2016
logOdds_dem2014 = logOdds_dem2016
pred
save(file = 'logOdds_dem2014.Rdata')
save(file = 'logOdds_dem2014.Rdata')
?save
logOdds_dem2014
save(logOdds_dem2014, file = 'logOdds_dem2014.Rdata')
load(file = 'data.Rdata')
pred2018 = read.csv(file = "pred2018.csv")
pred2014 = read.csv(file = "pred2014.csv")
load(file = 'logOdds_dem2014.Rdata')
set.seed(101)
indexing <- createDataPartition(y = data$y, p = 0.75,list = FALSE)
data_train = data[indexing,]
data_test = data[-indexing,]
knn_fit = train(y~.,
data = data_train,
trControl = trainControl(method = 'repeatedcv', number = 10, repeats = 3),
method = 'knn',
tuneLength = 10
)
knn_pred = predict(knn_fit, newdata = data_test)
knn_RMSE = sqrt(mean((data_test$y - knn_pred)^2))
boost.fit <- train(y ~ .,
data = data_train,
method = "gbm",
trControl = trainControl(method = 'repeatedcv', number = 10, repeats = 3),
verbose = FALSE,
tuneLength = 10
)
boost_pred = predict(boost.fit, newdata = data_test)
boost_RMSE = sqrt(mean((data_test$y - boost_pred)^2))
colnames(pred2014)
colnames(pred2018)
data2018 = pred2018[,c(-1,-2,-3)]
data2014 = pred2014[,c(-1,-2,-3)]
for (i in 1:ncol(data2018)){
data2018[,i] = as.numeric(data2018[,i])
}
data2018 = apply(data2018, 2, scale)
data2014 = apply(data2014, 2, scale)
data_pred = data2018 - data2014
data_pred
colnames(data_pred) = colnames(data)[-1]
pred = predict(boost.fit, newdata = data_pred)
pred+ logOdds_dem2014
result = pred + logOdds_dem2014
plot(density(result))
pred2018
data2018
pca_out = prcomp(data2018)
summary(pca_out)$importance[3,]
# From 1 to 12 principal components are used so that 97.49% is explained
data_cluster = pca_out$x[,1:6]
fviz_nbclust(data_cluster, kmeans, method = "wss", k.max = 10) # 4 looks good
fviz_nbclust(data_cluster, kmeans, method = "silhouette") # recommendation: 2
kmean_fit <- kmeans(data, centers = 2, nstart = 10)
plot(data_cluster,col=kmean_fit$cluster,cex=1.5,pch=16,lwd=2)
mix_out = Mclust(data_cluster, G = 2)
plot(data_cluster,col=mix_out$classification,cex=1.5,pch=16,lwd=2)
mix_out$z
fviz_nbclust(data_cluster, hcut, method = "silhouette", k.max = 20) # recommendation: 8
# Types of Linkage
m <- c( "average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")
ac <- function(x) {
agnes(data_cluster, method = x)$ac
}
map_dbl(m, ac)
hclust_ward = hcut(data_cluster, k = 2, hc_method = 'ward.D')
plot_ward = fviz_dend(hclust_ward, rect = TRUE, cex = 0.5,
k_colors = c("#00AFBB","#2E9FDF"))
plot_ward
boost_RMSE
knn_RMSE
rf_fit = train(y~.,
data = data_train,
trControl = trainControl(method = 'repeatedcv', number = 10, repeats = 3),
method = 'rf',
tuneLength = 5
)
rf_pred = predict(rf_fit, newdata = data_test)
rf_RMSE = sqrt(mean((data_test$y - rf_pred)^2))
bart_fit = gbart(x.train = data_train[,-1], y.train = data_train$y, type = 'wbart')
bart_pred = predict(bart_fit, newdata = data_test[,-1])
pred.mean=apply(bart_pred,2,mean)
bart_RMSE = sqrt(mean((data_test$y - pred.mean)^2))
RMSE = cbind(knn_RMSE, rf_RMSE, boost_RMSE, bart_RMSE)
RMSE
mix_out$z
a = c()
mix_out$z[,1]
a = c()
for (i in 1:nrow(mix_out$z)){
a[i] = abs(mix_out$z[,1] - mix_out$z[,2])
}
abs(mix_out$z[,1] - mix_out$z[,2])
a = c()
for (i in 1:nrow(mix_out$z)){
a[i] = abs(mix_out$z[,1] - mix_out$z[,2])
}
nrow(mix_out$z)
1:nrow(mix_out$z
)
i =1
mix_out$z[,1]
a = c()
for (i in 1:nrow(mix_out$z)){
a[i] = abs(mix_out$z[i,1] - mix_out$z[i,2])
}
a
plot(density(a))
which(a > 0.3)
which(a < 0.3)
length(which(a < 0.3))
fviz_nbclust(data_cluster, kmeans, method = "wss", k.max = 10) # 4 looks good
fviz_nbclust(data_cluster, kmeans, method = "wss", k.max = 10) # 4 looks good
fviz_nbclust(data_cluster, kmeans, method = "silhouette") # recommendation: 2
fviz_nbclust(data_cluster, kmeans, method = "silhouette") # recommendation: 2
kmean_fit <- kmeans(data, centers = 2, nstart = 10)
plot(data_cluster,col=kmean_fit$cluster,cex=1.5,pch=16,lwd=2)
View(data_train)
lm_fit = lm(y~., data = data_train)
plot(lm_fit)
plot(data_train)
plot(density(data_train))
plot(density(data_train[,1]))
plot(lm_fit)
View(data_train)
View(data)
View(data2018)
setwd("/Users/sangchanlee/Downloads/election_combined/finalResult")
load(file = 'data.Rdata')
pred2018 = read.csv(file = "pred2018.csv")
pred2014 = read.csv(file = "pred2014.csv")
load(file = 'logOdds_dem2014.Rdata')
View(pred2018)
pred2018$State
unique(pred2018$State)
?coef
fit.lasso = cv.glmnet(x = as.matrix(X), y = y, family = "gaussian")
full_data = full_data[-632,]
{
rm(list = ls())
setwd("/Users/sangchanlee/Downloads/election_combined")
sang_election = read.csv(file = "election_couty_cleaned.csv")
load(file = "full_data.Rdata")
}
# full_data: 2016 - 2012
full_data = full_data[-632,]
sang_election= sang_election[-623,]
# matching to common county
m1 = (which(full_data[,3] %in% sang_election[,4]))
data_feature_match = full_data[m1,]
data_feature_match = data_feature_match %>% arrange(State, Geography)
m2 = (which(sang_election[,4] %in% full_data[,3]))
data_response_match = sang_election[m2,]
data_response_match = sang_election[-729,]
data_response_match = data_response_match %>% arrange(state, county_nam)
states = c(rep('Colorado', 64), rep('Florida', 67), rep('Iowa', 99), rep('Michigan', 83), rep('Minnesota', 87),
rep('North Carolina', 100), rep('New Hampshire', 10), rep('Nevada', 17), rep('Ohio', 88),
rep('Pennsylvania', 66), rep('Virginia', 99), rep('Wisconsin', 72))
data_response_match$state = states
data_response_match = data_response_match %>% arrange(state, county_nam)
data_response_match = data_response_match[-691,]
data_feature_match = data_feature_match[-713,]
b = c()
for (i in 1:nrow(data_feature_match)){
b[i] = data_feature_match$Geography[i] != data_response_match$county_nam[i]
}
b
data_feature_match = data_feature_match[-620,]
data_response_match = data_response_match[-620,]
# extracting response for common county
data_response_match = data.frame(data_response_match[,3:4] ,data_response_match[,59:74])
# log odds
logOdds_dem2016 = log(data_response_match$pct_dem_16/(1-data_response_match$pct_dem_16))
logOdds_dem2012 = log(data_response_match$pct_dem_12/(1-data_response_match$pct_dem_12))
y = logOdds_dem2016 - logOdds_dem2012
# y = scale(y)
# y = abs(data_response_match$Demvotes16 - data_response_match$Demvotes12)
# y = scale(y)
X = data_feature_match[,-1:-4]
X = apply(X, 2, scale)
# set.seed(101)
# cv_lasso = cv.glmnet(X, y)
# lasso_coef = coef(cv_lasso, s = "lambda.min")
# lasso_vs = X[,-which(lasso_coef[-1,1]==0)]
# Lasso Regression for variable selection
set.seed(101)
fit.lasso = cv.glmnet(x = as.matrix(X), y = y, family = "gaussian")
coef_lasso = coef(fit.lasso, s = "lambda.1se")
?cv.glmnet
rm(list=ls())
setwd("/Users/sangchanlee/Downloads/election_combined/finalResult")
load(file = 'data.Rdata')
pred2018 = read.csv(file = "pred2018.csv")
pred2014 = read.csv(file = "pred2014.csv")
load(file = 'logOdds_dem2014.Rdata')
knn_fit = train(y~.,
data = data_train,
trControl = trainControl(method = 'repeatedcv', number = 10, repeats = 3),
method = 'knn',
tuneLength = 10
)
knn_RMSE = sqrt(mean((data_test$y - knn_pred)^2))
set.seed(101)
indexing <- createDataPartition(y = data$y, p = 0.75,list = FALSE)
data_train = data[indexing,]
data_test = data[-indexing,]
knn_fit = train(y~.,
data = data_train,
trControl = trainControl(method = 'repeatedcv', number = 10, repeats = 3),
method = 'knn',
tuneLength = 10
)
knn_fit = train(y~.,
data = data_train,
trControl = trainControl(method = 'repeatedcv', number = 10, repeats = 3),
method = 'knn',
tuneLength = 10
)
knn_pred = predict(knn_fit, newdata = data_test)
knn_RMSE = sqrt(mean((data_test$y - knn_pred)^2))
knn_fit
rf_fit = train(y~.,
data = data_train,
trControl = trainControl(method = 'repeatedcv', number = 10, repeats = 3),
method = 'rf',
tuneLength = 5
)
rf_pred = predict(rf_fit, newdata = data_test)
rf_RMSE = sqrt(mean((data_test$y - rf_pred)^2))
rf_fit
rf_pred = predict(rf_fit, newdata = data_test)
rf_RMSE = sqrt(mean((data_test$y - rf_pred)^2))
?rf
randomForest
?randomForest
boost.fit <- train(y ~ .,
data = data_train,
method = "gbm",
trControl = trainControl(method = 'repeatedcv', number = 10, repeats = 3),
verbose = FALSE,
tuneLength = 10
)
boost_pred = predict(boost.fit, newdata = data_test)
boost_RMSE = sqrt(mean((data_test$y - boost_pred)^2))
?scale
boost.fit
# Gradient Boost
?gbm
bart_fit = gbart(x.train = data_train[,-1], y.train = data_train$y, type = 'wbart')
bart_pred = predict(bart_fit, newdata = data_test[,-1])
pred.mean=apply(bart_pred,2,mean)
bart_RMSE = sqrt(mean((data_test$y - pred.mean)^2))
bart_fit
bart_fit = gbart(x.train = data_train[,-1], y.train = data_train$y, type = 'wbart')
bart_pred = predict(bart_fit, newdata = data_test[,-1])
pred.mean=apply(bart_pred,2,mean)
bart_RMSE = sqrt(mean((data_test$y - pred.mean)^2))
RMSE = cbind(knn_RMSE, rf_RMSE, boost_RMSE, bart_RMSE)
RMSE
